{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aff1fa35-7c52-4dc4-b39d-417e668644ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jupyter Notebook is working!\n"
     ]
    }
   ],
   "source": [
    "print(\"Jupyter Notebook is working!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c18ae323-ccd9-4839-8177-aa59dc928058",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in c:\\users\\patil\\anaconda3\\lib\\site-packages (2.2.2)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\patil\\anaconda3\\lib\\site-packages (1.6.1)\n",
      "Requirement already satisfied: nltk in c:\\users\\patil\\anaconda3\\lib\\site-packages (3.9.1)\n",
      "Requirement already satisfied: numpy>=1.26.0 in c:\\users\\patil\\anaconda3\\lib\\site-packages (from pandas) (1.26.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\patil\\anaconda3\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\patil\\anaconda3\\lib\\site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\patil\\anaconda3\\lib\\site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: scipy>=1.6.0 in c:\\users\\patil\\anaconda3\\lib\\site-packages (from scikit-learn) (1.13.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\patil\\anaconda3\\lib\\site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\patil\\anaconda3\\lib\\site-packages (from scikit-learn) (3.5.0)\n",
      "Requirement already satisfied: click in c:\\users\\patil\\anaconda3\\lib\\site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\patil\\anaconda3\\lib\\site-packages (from nltk) (2024.9.11)\n",
      "Requirement already satisfied: tqdm in c:\\users\\patil\\anaconda3\\lib\\site-packages (from nltk) (4.66.5)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\patil\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\patil\\anaconda3\\lib\\site-packages (from click->nltk) (0.4.6)\n"
     ]
    }
   ],
   "source": [
    "!pip install pandas scikit-learn nltk\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1adef454-b1b4-4005-b05d-398b5747bbaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\patil\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import nltk\n",
    "import string\n",
    "\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "79c60fa0-9332-4524-83ed-701b4624d844",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>message</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>You’ve been selected to receive a $1000 gift c...</td>\n",
       "      <td>spam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Hi, did you get a chance to look at the report...</td>\n",
       "      <td>ham</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Urgent! Your PayPal account has been locked. P...</td>\n",
       "      <td>spam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Let me know if you're free for a quick call to...</td>\n",
       "      <td>ham</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Limited time offer! 60% off on all items at Be...</td>\n",
       "      <td>spam</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             message label\n",
       "0  You’ve been selected to receive a $1000 gift c...  spam\n",
       "1  Hi, did you get a chance to look at the report...   ham\n",
       "2  Urgent! Your PayPal account has been locked. P...  spam\n",
       "3  Let me know if you're free for a quick call to...   ham\n",
       "4  Limited time offer! 60% off on all items at Be...  spam"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_json(\"whatsapp_spam_collection_dataset.json\")\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fc4bf554-c1f6-4ed0-a8f2-be650fcca4a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>message</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>You’ve been selected to receive a $1000 gift c...</td>\n",
       "      <td>spam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Hi, did you get a chance to look at the report...</td>\n",
       "      <td>ham</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Urgent! Your PayPal account has been locked. P...</td>\n",
       "      <td>spam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Let me know if you're free for a quick call to...</td>\n",
       "      <td>ham</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Limited time offer! 60% off on all items at Be...</td>\n",
       "      <td>spam</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             message label\n",
       "0  You’ve been selected to receive a $1000 gift c...  spam\n",
       "1  Hi, did you get a chance to look at the report...   ham\n",
       "2  Urgent! Your PayPal account has been locked. P...  spam\n",
       "3  Let me know if you're free for a quick call to...   ham\n",
       "4  Limited time offer! 60% off on all items at Be...  spam"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "81d63b94-fbbb-4e7c-9800-3c1418ccf6f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 181 entries, 0 to 180\n",
      "Data columns (total 2 columns):\n",
      " #   Column   Non-Null Count  Dtype \n",
      "---  ------   --------------  ----- \n",
      " 0   message  181 non-null    object\n",
      " 1   label    180 non-null    object\n",
      "dtypes: object(2)\n",
      "memory usage: 3.0+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1721bd2c-4ea0-4309-a529-ef7d541e509d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "message    0\n",
       "label      1\n",
       "dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5dd471ec-02f7-4996-8103-ca66d194b3f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label\n",
       "spam    91\n",
       "ham     89\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['label'].value_counts()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ec819c71-33ad-48ca-be05-e6417b9e96f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['message'] = df['message'].str.lower()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "76cbf83a-5271-49ac-91a7-aa56a9f9a0d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "df['message'] = df['message'].apply(lambda x: re.sub(r'[^a-zA-Z0-9\\s]', '', x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ebe6ac27-3616-43e0-9aca-eec29de28fc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\patil\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "nltk.download('stopwords')  # Download stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "df['message'] = df['message'].apply(lambda x: ' '.join(word for word in x.split() if word not in stop_words))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "39b1d144-18ad-46f0-a41a-d05a0b006eda",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['label'] = df['label'].map({'spam': 1, 'ham': 0})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "76c37f09-9e31-435b-a873-aafccf86ed50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X: (181, 301)\n",
      "Shape of y: (181,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Initialize TF-IDF Vectorizer\n",
    "vectorizer = TfidfVectorizer(max_features=5000)  # Limiting to 5000 features\n",
    "\n",
    "# Transform the messages\n",
    "X = vectorizer.fit_transform(df['message']).toarray()\n",
    "\n",
    "# Store the labels (Spam = 1, Ham = 0)\n",
    "y = df['label'].values\n",
    "\n",
    "# Print the shape to check dimensions\n",
    "print(\"Shape of X:\", X.shape)\n",
    "print(\"Shape of y:\", y.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "77f160b8-d779-4630-a950-943ffd717f7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set shape: (144, 301) (144,)\n",
      "Testing set shape: (37, 301) (37,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Splitting data into 80% training and 20% testing\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Print the shapes to verify\n",
    "print(\"Training set shape:\", X_train.shape, y_train.shape)\n",
    "print(\"Testing set shape:\", X_test.shape, y_test.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3571ac98-ea0b-4a7e-9ce8-4a7e0f0670af",
   "metadata": {},
   "source": [
    "## Training the Model using Naïve Bayes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4fee5051-8a42-42d2-91b3-6ad1aeba4267",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Input y contains NaN.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 7\u001b[0m\n\u001b[0;32m      4\u001b[0m nb_classifier \u001b[38;5;241m=\u001b[39m MultinomialNB()\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# Train (fit) the model using the training data\u001b[39;00m\n\u001b[1;32m----> 7\u001b[0m nb_classifier\u001b[38;5;241m.\u001b[39mfit(X_train, y_train)\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# Check the model's accuracy on training data\u001b[39;00m\n\u001b[0;32m     10\u001b[0m train_accuracy \u001b[38;5;241m=\u001b[39m nb_classifier\u001b[38;5;241m.\u001b[39mscore(X_train, y_train)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:1389\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1382\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1384\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1385\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1386\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1387\u001b[0m     )\n\u001b[0;32m   1388\u001b[0m ):\n\u001b[1;32m-> 1389\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\naive_bayes.py:735\u001b[0m, in \u001b[0;36m_BaseDiscreteNB.fit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    714\u001b[0m \u001b[38;5;129m@_fit_context\u001b[39m(prefer_skip_nested_validation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m    715\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfit\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, y, sample_weight\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m    716\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Fit Naive Bayes classifier according to X, y.\u001b[39;00m\n\u001b[0;32m    717\u001b[0m \n\u001b[0;32m    718\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    733\u001b[0m \u001b[38;5;124;03m        Returns the instance itself.\u001b[39;00m\n\u001b[0;32m    734\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 735\u001b[0m     X, y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_X_y(X, y)\n\u001b[0;32m    736\u001b[0m     _, n_features \u001b[38;5;241m=\u001b[39m X\u001b[38;5;241m.\u001b[39mshape\n\u001b[0;32m    738\u001b[0m     labelbin \u001b[38;5;241m=\u001b[39m LabelBinarizer()\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\naive_bayes.py:581\u001b[0m, in \u001b[0;36m_BaseDiscreteNB._check_X_y\u001b[1;34m(self, X, y, reset)\u001b[0m\n\u001b[0;32m    579\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_check_X_y\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, y, reset\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[0;32m    580\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Validate X and y in fit methods.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 581\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m validate_data(\u001b[38;5;28mself\u001b[39m, X, y, accept_sparse\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcsr\u001b[39m\u001b[38;5;124m\"\u001b[39m, reset\u001b[38;5;241m=\u001b[39mreset)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\validation.py:2961\u001b[0m, in \u001b[0;36mvalidate_data\u001b[1;34m(_estimator, X, y, reset, validate_separately, skip_check_array, **check_params)\u001b[0m\n\u001b[0;32m   2959\u001b[0m         y \u001b[38;5;241m=\u001b[39m check_array(y, input_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_y_params)\n\u001b[0;32m   2960\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 2961\u001b[0m         X, y \u001b[38;5;241m=\u001b[39m check_X_y(X, y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_params)\n\u001b[0;32m   2962\u001b[0m     out \u001b[38;5;241m=\u001b[39m X, y\n\u001b[0;32m   2964\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m check_params\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mensure_2d\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m):\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\validation.py:1387\u001b[0m, in \u001b[0;36mcheck_X_y\u001b[1;34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_writeable, force_all_finite, ensure_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, estimator)\u001b[0m\n\u001b[0;32m   1368\u001b[0m ensure_all_finite \u001b[38;5;241m=\u001b[39m _deprecate_force_all_finite(force_all_finite, ensure_all_finite)\n\u001b[0;32m   1370\u001b[0m X \u001b[38;5;241m=\u001b[39m check_array(\n\u001b[0;32m   1371\u001b[0m     X,\n\u001b[0;32m   1372\u001b[0m     accept_sparse\u001b[38;5;241m=\u001b[39maccept_sparse,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1384\u001b[0m     input_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   1385\u001b[0m )\n\u001b[1;32m-> 1387\u001b[0m y \u001b[38;5;241m=\u001b[39m _check_y(y, multi_output\u001b[38;5;241m=\u001b[39mmulti_output, y_numeric\u001b[38;5;241m=\u001b[39my_numeric, estimator\u001b[38;5;241m=\u001b[39mestimator)\n\u001b[0;32m   1389\u001b[0m check_consistent_length(X, y)\n\u001b[0;32m   1391\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m X, y\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\validation.py:1409\u001b[0m, in \u001b[0;36m_check_y\u001b[1;34m(y, multi_output, y_numeric, estimator)\u001b[0m\n\u001b[0;32m   1407\u001b[0m     estimator_name \u001b[38;5;241m=\u001b[39m _check_estimator_name(estimator)\n\u001b[0;32m   1408\u001b[0m     y \u001b[38;5;241m=\u001b[39m column_or_1d(y, warn\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m-> 1409\u001b[0m     _assert_all_finite(y, input_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my\u001b[39m\u001b[38;5;124m\"\u001b[39m, estimator_name\u001b[38;5;241m=\u001b[39mestimator_name)\n\u001b[0;32m   1410\u001b[0m     _ensure_no_complex_data(y)\n\u001b[0;32m   1411\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m y_numeric \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(y\u001b[38;5;241m.\u001b[39mdtype, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mkind\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m y\u001b[38;5;241m.\u001b[39mdtype\u001b[38;5;241m.\u001b[39mkind \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mO\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\validation.py:120\u001b[0m, in \u001b[0;36m_assert_all_finite\u001b[1;34m(X, allow_nan, msg_dtype, estimator_name, input_name)\u001b[0m\n\u001b[0;32m    117\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m first_pass_isfinite:\n\u001b[0;32m    118\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m--> 120\u001b[0m _assert_all_finite_element_wise(\n\u001b[0;32m    121\u001b[0m     X,\n\u001b[0;32m    122\u001b[0m     xp\u001b[38;5;241m=\u001b[39mxp,\n\u001b[0;32m    123\u001b[0m     allow_nan\u001b[38;5;241m=\u001b[39mallow_nan,\n\u001b[0;32m    124\u001b[0m     msg_dtype\u001b[38;5;241m=\u001b[39mmsg_dtype,\n\u001b[0;32m    125\u001b[0m     estimator_name\u001b[38;5;241m=\u001b[39mestimator_name,\n\u001b[0;32m    126\u001b[0m     input_name\u001b[38;5;241m=\u001b[39minput_name,\n\u001b[0;32m    127\u001b[0m )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\validation.py:169\u001b[0m, in \u001b[0;36m_assert_all_finite_element_wise\u001b[1;34m(X, xp, allow_nan, msg_dtype, estimator_name, input_name)\u001b[0m\n\u001b[0;32m    152\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m estimator_name \u001b[38;5;129;01mand\u001b[39;00m input_name \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m has_nan_error:\n\u001b[0;32m    153\u001b[0m     \u001b[38;5;66;03m# Improve the error message on how to handle missing values in\u001b[39;00m\n\u001b[0;32m    154\u001b[0m     \u001b[38;5;66;03m# scikit-learn.\u001b[39;00m\n\u001b[0;32m    155\u001b[0m     msg_err \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    156\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mestimator_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m does not accept missing values\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    157\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m encoded as NaN natively. For supervised learning, you might want\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    167\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m#estimators-that-handle-nan-values\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    168\u001b[0m     )\n\u001b[1;32m--> 169\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg_err)\n",
      "\u001b[1;31mValueError\u001b[0m: Input y contains NaN."
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "# Initialize the model\n",
    "nb_classifier = MultinomialNB()\n",
    "\n",
    "# Train (fit) the model using the training data\n",
    "nb_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Check the model's accuracy on training data\n",
    "train_accuracy = nb_classifier.score(X_train, y_train)\n",
    "print(\"Training Accuracy:\", train_accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "606ae8b3-1038-47c7-aa54-735f9b41343e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1.  1.  0.  1.  0.  1.  0.  0.  1.  0.  1.  0.  1.  0.  1.  1.  1.  1.\n",
      "  1.  1.  0.  0.  1.  0.  0.  1.  0.  1.  1.  1.  0.  1.  1.  0.  1.  0.\n",
      "  0.  1.  1.  1.  1.  1.  1.  0.  0.  1.  1.  0.  0.  1.  0.  1.  1.  1.\n",
      "  0.  0.  0.  1.  1.  1.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  1.  1.\n",
      "  1.  0.  0.  0.  0.  0.  1.  1.  1.  1.  0.  0.  1.  0.  0.  0.  0.  0.\n",
      "  1.  1.  0.  0.  1.  1.  1.  0.  1.  1.  0.  1.  1.  0.  0.  0.  1.  1.\n",
      "  1.  0.  1.  1.  1.  1.  0.  1.  1.  0.  0.  0.  1.  0.  0.  0.  1.  0.\n",
      "  1.  0.  1.  0.  1.  1.  0.  1.  0.  0. nan  1.  1.  0.  1.  0.  0.  0.]\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "print(y_train)\n",
    "print(pd.isnull(y_train).sum())  # Count NaN values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "cee22fd3-a248-40ff-88fd-1ac41d2dee67",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "invalid index to scalar variable.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[27], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mscipy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mstats\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m mode\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Find the most common label (0 or 1)\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m most_common_label \u001b[38;5;241m=\u001b[39m mode(y_train, nan_policy\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124momit\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mmode[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# Fill NaN values in y_train\u001b[39;00m\n\u001b[0;32m      7\u001b[0m y_train \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mnan_to_num(y_train, nan\u001b[38;5;241m=\u001b[39mmost_common_label)\n",
      "\u001b[1;31mIndexError\u001b[0m: invalid index to scalar variable."
     ]
    }
   ],
   "source": [
    "from scipy.stats import mode\n",
    "\n",
    "# Find the most common label (0 or 1)\n",
    "most_common_label = mode(y_train, nan_policy=\"omit\").mode[0]\n",
    "\n",
    "# Fill NaN values in y_train\n",
    "y_train = np.nan_to_num(y_train, nan=most_common_label)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4e1ddd20-f46d-4433-ade5-1a0657ac9711",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import mode\n",
    "import numpy as np\n",
    "\n",
    "# Find the most common label (0 or 1)\n",
    "most_common_label = mode(y_train, nan_policy=\"omit\").mode\n",
    "\n",
    "# Fill NaN values in y_train\n",
    "y_train = np.nan_to_num(y_train, nan=most_common_label)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "dfc3a7e3-f8c9-4399-9b94-fecd24ff9e46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "# Initialize the Naive Bayes classifier\n",
    "nb_classifier = MultinomialNB()\n",
    "\n",
    "# Train (fit) the model using the training data\n",
    "nb_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Check the model's accuracy on training data\n",
    "train_accuracy = nb_classifier.score(X_train, y_train)\n",
    "print(\"Training Accuracy:\", train_accuracy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ea734d5-9c63-43b8-aee8-a2f55bf19063",
   "metadata": {},
   "source": [
    "## Test the Model on Unseen Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e9b5378a-e9ce-46e1-ae6f-a0d0bf114674",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "# Check the model's accuracy on test data\n",
    "test_accuracy = nb_classifier.score(X_test, y_test)\n",
    "print(\"Testing Accuracy:\", test_accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d254b10c-f949-44cb-a6be-203ec5b82ea9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actual: 0.0 - Predicted: 0.0\n",
      "Actual: 1.0 - Predicted: 1.0\n",
      "Actual: 0.0 - Predicted: 0.0\n",
      "Actual: 0.0 - Predicted: 0.0\n",
      "Actual: 0.0 - Predicted: 0.0\n",
      "Actual: 0.0 - Predicted: 0.0\n",
      "Actual: 1.0 - Predicted: 1.0\n",
      "Actual: 0.0 - Predicted: 0.0\n",
      "Actual: 1.0 - Predicted: 1.0\n",
      "Actual: 0.0 - Predicted: 0.0\n"
     ]
    }
   ],
   "source": [
    "# Predict labels for test data\n",
    "y_pred = nb_classifier.predict(X_test)\n",
    "\n",
    "# Print some predictions alongside actual labels\n",
    "for i in range(10):  # Show first 10 examples\n",
    "    print(f\"Actual: {y_test[i]} - Predicted: {y_pred[i]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b4fd1aa1-d37d-4f63-a520-4124314ee5d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      1.00      1.00        19\n",
      "         1.0       1.00      1.00      1.00        18\n",
      "\n",
      "    accuracy                           1.00        37\n",
      "   macro avg       1.00      1.00      1.00        37\n",
      "weighted avg       1.00      1.00      1.00        37\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Generate report\n",
    "print(classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "a9f754d7-bc68-4387-95c2-aa505530f134",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     v1                                                 v2 Unnamed: 2  \\\n",
      "0   ham  Go until jurong point, crazy.. Available only ...        NaN   \n",
      "1   ham                      Ok lar... Joking wif u oni...        NaN   \n",
      "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...        NaN   \n",
      "3   ham  U dun say so early hor... U c already then say...        NaN   \n",
      "4   ham  Nah I don't think he goes to usf, he lives aro...        NaN   \n",
      "\n",
      "  Unnamed: 3 Unnamed: 4  \n",
      "0        NaN        NaN  \n",
      "1        NaN        NaN  \n",
      "2        NaN        NaN  \n",
      "3        NaN        NaN  \n",
      "4        NaN        NaN  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "import joblib\n",
    "\n",
    "# Load dataset with encoding\n",
    "df = pd.read_csv(\"spam.csv\", encoding=\"latin1\")\n",
    "\n",
    "# Display first few rows\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "a1983fe1-bed0-4caa-946a-9dd6951695d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  label                                            message\n",
      "0   ham  Go until jurong point, crazy.. Available only ...\n",
      "1   ham                      Ok lar... Joking wif u oni...\n",
      "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
      "3   ham  U dun say so early hor... U c already then say...\n",
      "4   ham  Nah I don't think he goes to usf, he lives aro...\n"
     ]
    }
   ],
   "source": [
    "# Rename columns for clarity\n",
    "df = df.rename(columns={\"v1\": \"label\", \"v2\": \"message\"})\n",
    "\n",
    "# Drop unnecessary columns\n",
    "df = df[[\"label\", \"message\"]]\n",
    "\n",
    "# Display the cleaned dataset\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "2fae1917-1565-4b24-bae0-1683d9b5ce5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label      0\n",
      "message    0\n",
      "dtype: int64\n",
      "Duplicate messages: 403\n",
      "label\n",
      "ham     4825\n",
      "spam     747\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Check for missing values\n",
    "print(df.isnull().sum())\n",
    "\n",
    "# Check for duplicate messages\n",
    "print(\"Duplicate messages:\", df.duplicated().sum())\n",
    "\n",
    "# Check class distribution\n",
    "print(df['label'].value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "93b475ce-8dea-45bc-b72e-95fbe73c8b14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Remaining messages after duplicate removal: 5169\n"
     ]
    }
   ],
   "source": [
    "# Remove duplicate messages\n",
    "df = df.drop_duplicates(subset=\"message\")\n",
    "\n",
    "# Verify\n",
    "print(\"Remaining messages after duplicate removal:\", df.shape[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "52664c37-f9ba-410f-9108-9a84505c4272",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label      0\n",
      "message    0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(df.isnull().sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "bca470c3-0779-436f-b9b2-20437601e31e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   label                                            message\n",
      "0      0  Go until jurong point, crazy.. Available only ...\n",
      "1      0                      Ok lar... Joking wif u oni...\n",
      "2      1  Free entry in 2 a wkly comp to win FA Cup fina...\n",
      "3      0  U dun say so early hor... U c already then say...\n",
      "4      0  Nah I don't think he goes to usf, he lives aro...\n"
     ]
    }
   ],
   "source": [
    "df[\"label\"] = df[\"label\"].map({\"ham\": 0, \"spam\": 1})\n",
    "\n",
    "# Verify\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abe510a0-0ac5-454f-bc9b-9c7084592dc7",
   "metadata": {},
   "source": [
    "### text preprocessing - lowercase , removing special characters etc etc "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "6dbc4412-c51c-4816-92f2-42499db6be10",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\patil\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                             message  \\\n",
      "0  Go until jurong point, crazy.. Available only ...   \n",
      "1                      Ok lar... Joking wif u oni...   \n",
      "2  Free entry in 2 a wkly comp to win FA Cup fina...   \n",
      "3  U dun say so early hor... U c already then say...   \n",
      "4  Nah I don't think he goes to usf, he lives aro...   \n",
      "\n",
      "                                     cleaned_message  \n",
      "0  go jurong point crazi avail bugi n great world...  \n",
      "1                              ok lar joke wif u oni  \n",
      "2  free entri wkli comp win fa cup final tkt st m...  \n",
      "3                u dun say earli hor u c alreadi say  \n",
      "4               nah think goe usf live around though  \n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "# Download stopwords if not already downloaded\n",
    "nltk.download(\"stopwords\")\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "\n",
    "def preprocess_text(text):\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    # Remove special characters and numbers\n",
    "    text = re.sub(r\"[^a-zA-Z]\", \" \", text)\n",
    "    # Tokenize words\n",
    "    words = text.split()\n",
    "    # Remove stopwords and apply stemming\n",
    "    words = [stemmer.stem(word) for word in words if word not in stop_words]\n",
    "    # Join words back into a string\n",
    "    return \" \".join(words)\n",
    "\n",
    "# Apply preprocessing\n",
    "df[\"cleaned_message\"] = df[\"message\"].apply(preprocess_text)\n",
    "\n",
    "# Verify\n",
    "print(df[[\"message\", \"cleaned_message\"]].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "da00b9b6-2bf3-4bd6-b730-cb98be33ffde",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(df['cleaned_message'], df['label'], test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "8e11e7d8-477c-4b95-8494-968295996d17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      1.00      0.98       889\n",
      "           1       0.99      0.78      0.87       145\n",
      "\n",
      "    accuracy                           0.97      1034\n",
      "   macro avg       0.98      0.89      0.93      1034\n",
      "weighted avg       0.97      0.97      0.97      1034\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Convert text into TF-IDF features\n",
    "vectorizer = TfidfVectorizer()\n",
    "X_train_tfidf = vectorizer.fit_transform(X_train)\n",
    "X_test_tfidf = vectorizer.transform(X_test)\n",
    "\n",
    "# Train a new Naive Bayes classifier\n",
    "nb_classifier = MultinomialNB()\n",
    "nb_classifier.fit(X_train_tfidf, y_train)\n",
    "\n",
    "# Evaluate the model\n",
    "y_pred = nb_classifier.predict(X_test_tfidf)\n",
    "print(classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "bb3ba3b8-e406-4d40-9df3-d25aceeea7b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated model and vectorizer saved successfully!\n"
     ]
    }
   ],
   "source": [
    "# Save the trained classifier and TF-IDF vectorizer\n",
    "joblib.dump(nb_classifier, \"spam_classifier.pkl\")\n",
    "joblib.dump(vectorizer, \"tfidf_vectorizer.pkl\")\n",
    "\n",
    "print(\"Updated model and vectorizer saved successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "5af37498-ee18-424a-bdd5-d679d1696b1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spam\n"
     ]
    }
   ],
   "source": [
    "# Load the updated model and vectorizer\n",
    "loaded_classifier = joblib.load(\"spam_classifier.pkl\")\n",
    "loaded_vectorizer = joblib.load(\"tfidf_vectorizer.pkl\")\n",
    "\n",
    "# Example new message\n",
    "new_message = [\"Congratulations! You won a free iPhone! Claim now.\"]\n",
    "new_message_transformed = loaded_vectorizer.transform(new_message)\n",
    "\n",
    "# Predict\n",
    "prediction = loaded_classifier.predict(new_message_transformed)\n",
    "\n",
    "# Output result\n",
    "print(\"Spam\" if prediction[0] == 1 else \"Ham\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "c35d8039-3dc7-4e14-96f5-7223976de962",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ham\n"
     ]
    }
   ],
   "source": [
    "# Load the updated model and vectorizer\n",
    "loaded_classifier = joblib.load(\"spam_classifier.pkl\")\n",
    "loaded_vectorizer = joblib.load(\"tfidf_vectorizer.pkl\")\n",
    "\n",
    "# Example new message\n",
    "new_message = [\"hey what are you doing?\"]\n",
    "new_message_transformed = loaded_vectorizer.transform(new_message)\n",
    "\n",
    "# Predict\n",
    "prediction = loaded_classifier.predict(new_message_transformed)\n",
    "\n",
    "# Output result\n",
    "print(\"Spam\" if prediction[0] == 1 else \"Ham\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "ac5da278-2d3c-428c-8c09-4d3c97a4e781",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.6.1\n"
     ]
    }
   ],
   "source": [
    "import sklearn\n",
    "print(sklearn.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "0d69ffda-d12f-4689-a4f8-142b90a2f5db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\patil\\anaconda3\\python.exe\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.executable)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "516639be-fe70-4de4-8174-ba2d7f1b2f01",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
