{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aff1fa35-7c52-4dc4-b39d-417e668644ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jupyter Notebook is working!\n"
     ]
    }
   ],
   "source": [
    "print(\"Jupyter Notebook is working!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c18ae323-ccd9-4839-8177-aa59dc928058",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in c:\\users\\patil\\anaconda3\\lib\\site-packages (2.2.2)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\patil\\anaconda3\\lib\\site-packages (1.5.1)\n",
      "Requirement already satisfied: nltk in c:\\users\\patil\\anaconda3\\lib\\site-packages (3.9.1)\n",
      "Requirement already satisfied: numpy>=1.26.0 in c:\\users\\patil\\anaconda3\\lib\\site-packages (from pandas) (1.26.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\patil\\anaconda3\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\patil\\anaconda3\\lib\\site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\patil\\anaconda3\\lib\\site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: scipy>=1.6.0 in c:\\users\\patil\\anaconda3\\lib\\site-packages (from scikit-learn) (1.13.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\patil\\anaconda3\\lib\\site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\patil\\anaconda3\\lib\\site-packages (from scikit-learn) (3.5.0)\n",
      "Requirement already satisfied: click in c:\\users\\patil\\anaconda3\\lib\\site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\patil\\anaconda3\\lib\\site-packages (from nltk) (2024.9.11)\n",
      "Requirement already satisfied: tqdm in c:\\users\\patil\\anaconda3\\lib\\site-packages (from nltk) (4.66.5)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\patil\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\patil\\anaconda3\\lib\\site-packages (from click->nltk) (0.4.6)\n"
     ]
    }
   ],
   "source": [
    "!pip install pandas scikit-learn nltk\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1adef454-b1b4-4005-b05d-398b5747bbaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\patil\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import nltk\n",
    "import string\n",
    "\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "79c60fa0-9332-4524-83ed-701b4624d844",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>message</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>You’ve been selected to receive a $1000 gift c...</td>\n",
       "      <td>spam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Hi, did you get a chance to look at the report...</td>\n",
       "      <td>ham</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Urgent! Your PayPal account has been locked. P...</td>\n",
       "      <td>spam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Let me know if you're free for a quick call to...</td>\n",
       "      <td>ham</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Limited time offer! 60% off on all items at Be...</td>\n",
       "      <td>spam</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             message label\n",
       "0  You’ve been selected to receive a $1000 gift c...  spam\n",
       "1  Hi, did you get a chance to look at the report...   ham\n",
       "2  Urgent! Your PayPal account has been locked. P...  spam\n",
       "3  Let me know if you're free for a quick call to...   ham\n",
       "4  Limited time offer! 60% off on all items at Be...  spam"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_json(\"whatsapp_spam_collection_dataset.json\")\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fc4bf554-c1f6-4ed0-a8f2-be650fcca4a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>message</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>You’ve been selected to receive a $1000 gift c...</td>\n",
       "      <td>spam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Hi, did you get a chance to look at the report...</td>\n",
       "      <td>ham</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Urgent! Your PayPal account has been locked. P...</td>\n",
       "      <td>spam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Let me know if you're free for a quick call to...</td>\n",
       "      <td>ham</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Limited time offer! 60% off on all items at Be...</td>\n",
       "      <td>spam</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             message label\n",
       "0  You’ve been selected to receive a $1000 gift c...  spam\n",
       "1  Hi, did you get a chance to look at the report...   ham\n",
       "2  Urgent! Your PayPal account has been locked. P...  spam\n",
       "3  Let me know if you're free for a quick call to...   ham\n",
       "4  Limited time offer! 60% off on all items at Be...  spam"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "81d63b94-fbbb-4e7c-9800-3c1418ccf6f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 181 entries, 0 to 180\n",
      "Data columns (total 2 columns):\n",
      " #   Column   Non-Null Count  Dtype \n",
      "---  ------   --------------  ----- \n",
      " 0   message  181 non-null    object\n",
      " 1   label    180 non-null    object\n",
      "dtypes: object(2)\n",
      "memory usage: 3.0+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1721bd2c-4ea0-4309-a529-ef7d541e509d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "message    0\n",
       "label      1\n",
       "dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5dd471ec-02f7-4996-8103-ca66d194b3f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label\n",
       "spam    91\n",
       "ham     89\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['label'].value_counts()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ec819c71-33ad-48ca-be05-e6417b9e96f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['message'] = df['message'].str.lower()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "76cbf83a-5271-49ac-91a7-aa56a9f9a0d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "df['message'] = df['message'].apply(lambda x: re.sub(r'[^a-zA-Z0-9\\s]', '', x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ebe6ac27-3616-43e0-9aca-eec29de28fc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\patil\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "nltk.download('stopwords')  # Download stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "df['message'] = df['message'].apply(lambda x: ' '.join(word for word in x.split() if word not in stop_words))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "39b1d144-18ad-46f0-a41a-d05a0b006eda",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['label'] = df['label'].map({'spam': 1, 'ham': 0})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "76c37f09-9e31-435b-a873-aafccf86ed50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X: (181, 301)\n",
      "Shape of y: (181,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Initialize TF-IDF Vectorizer\n",
    "vectorizer = TfidfVectorizer(max_features=5000)  # Limiting to 5000 features\n",
    "\n",
    "# Transform the messages\n",
    "X = vectorizer.fit_transform(df['message']).toarray()\n",
    "\n",
    "# Store the labels (Spam = 1, Ham = 0)\n",
    "y = df['label'].values\n",
    "\n",
    "# Print the shape to check dimensions\n",
    "print(\"Shape of X:\", X.shape)\n",
    "print(\"Shape of y:\", y.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "77f160b8-d779-4630-a950-943ffd717f7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set shape: (144, 301) (144,)\n",
      "Testing set shape: (37, 301) (37,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Splitting data into 80% training and 20% testing\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Print the shapes to verify\n",
    "print(\"Training set shape:\", X_train.shape, y_train.shape)\n",
    "print(\"Testing set shape:\", X_test.shape, y_test.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3571ac98-ea0b-4a7e-9ce8-4a7e0f0670af",
   "metadata": {},
   "source": [
    "## Training the Model using Naïve Bayes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4fee5051-8a42-42d2-91b3-6ad1aeba4267",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Input y contains NaN.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[18], line 7\u001b[0m\n\u001b[0;32m      4\u001b[0m nb_classifier \u001b[38;5;241m=\u001b[39m MultinomialNB()\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# Train (fit) the model using the training data\u001b[39;00m\n\u001b[1;32m----> 7\u001b[0m nb_classifier\u001b[38;5;241m.\u001b[39mfit(X_train, y_train)\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# Check the model's accuracy on training data\u001b[39;00m\n\u001b[0;32m     10\u001b[0m train_accuracy \u001b[38;5;241m=\u001b[39m nb_classifier\u001b[38;5;241m.\u001b[39mscore(X_train, y_train)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:1473\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1466\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1468\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1469\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1470\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1471\u001b[0m     )\n\u001b[0;32m   1472\u001b[0m ):\n\u001b[1;32m-> 1473\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\naive_bayes.py:732\u001b[0m, in \u001b[0;36m_BaseDiscreteNB.fit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    711\u001b[0m \u001b[38;5;129m@_fit_context\u001b[39m(prefer_skip_nested_validation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m    712\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfit\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, y, sample_weight\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m    713\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Fit Naive Bayes classifier according to X, y.\u001b[39;00m\n\u001b[0;32m    714\u001b[0m \n\u001b[0;32m    715\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    730\u001b[0m \u001b[38;5;124;03m        Returns the instance itself.\u001b[39;00m\n\u001b[0;32m    731\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 732\u001b[0m     X, y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_X_y(X, y)\n\u001b[0;32m    733\u001b[0m     _, n_features \u001b[38;5;241m=\u001b[39m X\u001b[38;5;241m.\u001b[39mshape\n\u001b[0;32m    735\u001b[0m     labelbin \u001b[38;5;241m=\u001b[39m LabelBinarizer()\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\naive_bayes.py:578\u001b[0m, in \u001b[0;36m_BaseDiscreteNB._check_X_y\u001b[1;34m(self, X, y, reset)\u001b[0m\n\u001b[0;32m    576\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_check_X_y\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, y, reset\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[0;32m    577\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Validate X and y in fit methods.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 578\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_data(X, y, accept_sparse\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcsr\u001b[39m\u001b[38;5;124m\"\u001b[39m, reset\u001b[38;5;241m=\u001b[39mreset)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:650\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[1;34m(self, X, y, reset, validate_separately, cast_to_ndarray, **check_params)\u001b[0m\n\u001b[0;32m    648\u001b[0m         y \u001b[38;5;241m=\u001b[39m check_array(y, input_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_y_params)\n\u001b[0;32m    649\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 650\u001b[0m         X, y \u001b[38;5;241m=\u001b[39m check_X_y(X, y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_params)\n\u001b[0;32m    651\u001b[0m     out \u001b[38;5;241m=\u001b[39m X, y\n\u001b[0;32m    653\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m check_params\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mensure_2d\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m):\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\validation.py:1318\u001b[0m, in \u001b[0;36mcheck_X_y\u001b[1;34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_writeable, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, estimator)\u001b[0m\n\u001b[0;32m   1297\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1298\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mestimator_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m requires y to be passed, but the target y is None\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1299\u001b[0m     )\n\u001b[0;32m   1301\u001b[0m X \u001b[38;5;241m=\u001b[39m check_array(\n\u001b[0;32m   1302\u001b[0m     X,\n\u001b[0;32m   1303\u001b[0m     accept_sparse\u001b[38;5;241m=\u001b[39maccept_sparse,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1315\u001b[0m     input_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   1316\u001b[0m )\n\u001b[1;32m-> 1318\u001b[0m y \u001b[38;5;241m=\u001b[39m _check_y(y, multi_output\u001b[38;5;241m=\u001b[39mmulti_output, y_numeric\u001b[38;5;241m=\u001b[39my_numeric, estimator\u001b[38;5;241m=\u001b[39mestimator)\n\u001b[0;32m   1320\u001b[0m check_consistent_length(X, y)\n\u001b[0;32m   1322\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m X, y\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\validation.py:1340\u001b[0m, in \u001b[0;36m_check_y\u001b[1;34m(y, multi_output, y_numeric, estimator)\u001b[0m\n\u001b[0;32m   1338\u001b[0m     estimator_name \u001b[38;5;241m=\u001b[39m _check_estimator_name(estimator)\n\u001b[0;32m   1339\u001b[0m     y \u001b[38;5;241m=\u001b[39m column_or_1d(y, warn\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m-> 1340\u001b[0m     _assert_all_finite(y, input_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my\u001b[39m\u001b[38;5;124m\"\u001b[39m, estimator_name\u001b[38;5;241m=\u001b[39mestimator_name)\n\u001b[0;32m   1341\u001b[0m     _ensure_no_complex_data(y)\n\u001b[0;32m   1342\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m y_numeric \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(y\u001b[38;5;241m.\u001b[39mdtype, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mkind\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m y\u001b[38;5;241m.\u001b[39mdtype\u001b[38;5;241m.\u001b[39mkind \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mO\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\validation.py:123\u001b[0m, in \u001b[0;36m_assert_all_finite\u001b[1;34m(X, allow_nan, msg_dtype, estimator_name, input_name)\u001b[0m\n\u001b[0;32m    120\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m first_pass_isfinite:\n\u001b[0;32m    121\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m--> 123\u001b[0m _assert_all_finite_element_wise(\n\u001b[0;32m    124\u001b[0m     X,\n\u001b[0;32m    125\u001b[0m     xp\u001b[38;5;241m=\u001b[39mxp,\n\u001b[0;32m    126\u001b[0m     allow_nan\u001b[38;5;241m=\u001b[39mallow_nan,\n\u001b[0;32m    127\u001b[0m     msg_dtype\u001b[38;5;241m=\u001b[39mmsg_dtype,\n\u001b[0;32m    128\u001b[0m     estimator_name\u001b[38;5;241m=\u001b[39mestimator_name,\n\u001b[0;32m    129\u001b[0m     input_name\u001b[38;5;241m=\u001b[39minput_name,\n\u001b[0;32m    130\u001b[0m )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\validation.py:172\u001b[0m, in \u001b[0;36m_assert_all_finite_element_wise\u001b[1;34m(X, xp, allow_nan, msg_dtype, estimator_name, input_name)\u001b[0m\n\u001b[0;32m    155\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m estimator_name \u001b[38;5;129;01mand\u001b[39;00m input_name \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m has_nan_error:\n\u001b[0;32m    156\u001b[0m     \u001b[38;5;66;03m# Improve the error message on how to handle missing values in\u001b[39;00m\n\u001b[0;32m    157\u001b[0m     \u001b[38;5;66;03m# scikit-learn.\u001b[39;00m\n\u001b[0;32m    158\u001b[0m     msg_err \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    159\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mestimator_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m does not accept missing values\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    160\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m encoded as NaN natively. For supervised learning, you might want\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    170\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m#estimators-that-handle-nan-values\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    171\u001b[0m     )\n\u001b[1;32m--> 172\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg_err)\n",
      "\u001b[1;31mValueError\u001b[0m: Input y contains NaN."
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "# Initialize the model\n",
    "nb_classifier = MultinomialNB()\n",
    "\n",
    "# Train (fit) the model using the training data\n",
    "nb_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Check the model's accuracy on training data\n",
    "train_accuracy = nb_classifier.score(X_train, y_train)\n",
    "print(\"Training Accuracy:\", train_accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "606ae8b3-1038-47c7-aa54-735f9b41343e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1.  1.  0.  1.  0.  1.  0.  0.  1.  0.  1.  0.  1.  0.  1.  1.  1.  1.\n",
      "  1.  1.  0.  0.  1.  0.  0.  1.  0.  1.  1.  1.  0.  1.  1.  0.  1.  0.\n",
      "  0.  1.  1.  1.  1.  1.  1.  0.  0.  1.  1.  0.  0.  1.  0.  1.  1.  1.\n",
      "  0.  0.  0.  1.  1.  1.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  1.  1.\n",
      "  1.  0.  0.  0.  0.  0.  1.  1.  1.  1.  0.  0.  1.  0.  0.  0.  0.  0.\n",
      "  1.  1.  0.  0.  1.  1.  1.  0.  1.  1.  0.  1.  1.  0.  0.  0.  1.  1.\n",
      "  1.  0.  1.  1.  1.  1.  0.  1.  1.  0.  0.  0.  1.  0.  0.  0.  1.  0.\n",
      "  1.  0.  1.  0.  1.  1.  0.  1.  0.  0. nan  1.  1.  0.  1.  0.  0.  0.]\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "print(y_train)\n",
    "print(pd.isnull(y_train).sum())  # Count NaN values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "cee22fd3-a248-40ff-88fd-1ac41d2dee67",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "invalid index to scalar variable.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[27], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mscipy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mstats\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m mode\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Find the most common label (0 or 1)\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m most_common_label \u001b[38;5;241m=\u001b[39m mode(y_train, nan_policy\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124momit\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mmode[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# Fill NaN values in y_train\u001b[39;00m\n\u001b[0;32m      7\u001b[0m y_train \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mnan_to_num(y_train, nan\u001b[38;5;241m=\u001b[39mmost_common_label)\n",
      "\u001b[1;31mIndexError\u001b[0m: invalid index to scalar variable."
     ]
    }
   ],
   "source": [
    "from scipy.stats import mode\n",
    "\n",
    "# Find the most common label (0 or 1)\n",
    "most_common_label = mode(y_train, nan_policy=\"omit\").mode[0]\n",
    "\n",
    "# Fill NaN values in y_train\n",
    "y_train = np.nan_to_num(y_train, nan=most_common_label)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "4e1ddd20-f46d-4433-ade5-1a0657ac9711",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import mode\n",
    "import numpy as np\n",
    "\n",
    "# Find the most common label (0 or 1)\n",
    "most_common_label = mode(y_train, nan_policy=\"omit\").mode\n",
    "\n",
    "# Fill NaN values in y_train\n",
    "y_train = np.nan_to_num(y_train, nan=most_common_label)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "dfc3a7e3-f8c9-4399-9b94-fecd24ff9e46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "# Initialize the Naive Bayes classifier\n",
    "nb_classifier = MultinomialNB()\n",
    "\n",
    "# Train (fit) the model using the training data\n",
    "nb_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Check the model's accuracy on training data\n",
    "train_accuracy = nb_classifier.score(X_train, y_train)\n",
    "print(\"Training Accuracy:\", train_accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "1bab638e-9a4b-4710-b3e9-fff87761fea7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved as spam_classifier.pkl\n"
     ]
    }
   ],
   "source": [
    "import joblib\n",
    "\n",
    "# Save the trained Naive Bayes classifier\n",
    "joblib.dump(nb_classifier, \"spam_classifier.pkl\")\n",
    "print(\"Model saved as spam_classifier.pkl\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ea734d5-9c63-43b8-aee8-a2f55bf19063",
   "metadata": {},
   "source": [
    "## Test the Model on Unseen Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "e9b5378a-e9ce-46e1-ae6f-a0d0bf114674",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "# Check the model's accuracy on test data\n",
    "test_accuracy = nb_classifier.score(X_test, y_test)\n",
    "print(\"Testing Accuracy:\", test_accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "d254b10c-f949-44cb-a6be-203ec5b82ea9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actual: 0.0 - Predicted: 0.0\n",
      "Actual: 1.0 - Predicted: 1.0\n",
      "Actual: 0.0 - Predicted: 0.0\n",
      "Actual: 0.0 - Predicted: 0.0\n",
      "Actual: 0.0 - Predicted: 0.0\n",
      "Actual: 0.0 - Predicted: 0.0\n",
      "Actual: 1.0 - Predicted: 1.0\n",
      "Actual: 0.0 - Predicted: 0.0\n",
      "Actual: 1.0 - Predicted: 1.0\n",
      "Actual: 0.0 - Predicted: 0.0\n"
     ]
    }
   ],
   "source": [
    "# Predict labels for test data\n",
    "y_pred = nb_classifier.predict(X_test)\n",
    "\n",
    "# Print some predictions alongside actual labels\n",
    "for i in range(10):  # Show first 10 examples\n",
    "    print(f\"Actual: {y_test[i]} - Predicted: {y_pred[i]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "b4fd1aa1-d37d-4f63-a520-4124314ee5d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      1.00      1.00        19\n",
      "         1.0       1.00      1.00      1.00        18\n",
      "\n",
      "    accuracy                           1.00        37\n",
      "   macro avg       1.00      1.00      1.00        37\n",
      "weighted avg       1.00      1.00      1.00        37\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Generate report\n",
    "print(classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "88b1bc50-1c8f-4e6b-8ddd-4faea3a6e964",
   "metadata": {},
   "outputs": [
    {
     "ename": "UnicodeDecodeError",
     "evalue": "'utf-8' codec can't decode bytes in position 606-607: invalid continuation byte",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnicodeDecodeError\u001b[0m                        Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[50], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Load dataset \u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mspam.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# Display first few rows\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(df\u001b[38;5;241m.\u001b[39mhead())\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[0;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m   1014\u001b[0m     dialect,\n\u001b[0;32m   1015\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[0;32m   1023\u001b[0m )\n\u001b[0;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:620\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    617\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[0;32m    619\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[1;32m--> 620\u001b[0m parser \u001b[38;5;241m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    622\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[0;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1620\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m   1617\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m   1619\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1620\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_engine(f, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1898\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1895\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg)\n\u001b[0;32m   1897\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1898\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m mapping[engine](f, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions)\n\u001b[0;32m   1899\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[0;32m   1900\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\c_parser_wrapper.py:93\u001b[0m, in \u001b[0;36mCParserWrapper.__init__\u001b[1;34m(self, src, **kwds)\u001b[0m\n\u001b[0;32m     90\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype_backend\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpyarrow\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m     91\u001b[0m     \u001b[38;5;66;03m# Fail here loudly instead of in cython after reading\u001b[39;00m\n\u001b[0;32m     92\u001b[0m     import_optional_dependency(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpyarrow\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 93\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reader \u001b[38;5;241m=\u001b[39m parsers\u001b[38;5;241m.\u001b[39mTextReader(src, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m     95\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39munnamed_cols \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reader\u001b[38;5;241m.\u001b[39munnamed_cols\n\u001b[0;32m     97\u001b[0m \u001b[38;5;66;03m# error: Cannot determine type of 'names'\u001b[39;00m\n",
      "File \u001b[1;32mparsers.pyx:574\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mparsers.pyx:663\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._get_header\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mparsers.pyx:874\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._tokenize_rows\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mparsers.pyx:891\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._check_tokenize_status\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mparsers.pyx:2053\u001b[0m, in \u001b[0;36mpandas._libs.parsers.raise_parser_error\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32m<frozen codecs>:322\u001b[0m, in \u001b[0;36mdecode\u001b[1;34m(self, input, final)\u001b[0m\n",
      "\u001b[1;31mUnicodeDecodeError\u001b[0m: 'utf-8' codec can't decode bytes in position 606-607: invalid continuation byte"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load dataset \n",
    "df = pd.read_csv(\"spam.csv\")\n",
    "\n",
    "# Display first few rows\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "a9f754d7-bc68-4387-95c2-aa505530f134",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     v1                                                 v2 Unnamed: 2  \\\n",
      "0   ham  Go until jurong point, crazy.. Available only ...        NaN   \n",
      "1   ham                      Ok lar... Joking wif u oni...        NaN   \n",
      "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...        NaN   \n",
      "3   ham  U dun say so early hor... U c already then say...        NaN   \n",
      "4   ham  Nah I don't think he goes to usf, he lives aro...        NaN   \n",
      "\n",
      "  Unnamed: 3 Unnamed: 4  \n",
      "0        NaN        NaN  \n",
      "1        NaN        NaN  \n",
      "2        NaN        NaN  \n",
      "3        NaN        NaN  \n",
      "4        NaN        NaN  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load dataset with encoding\n",
    "df = pd.read_csv(\"spam.csv\", encoding=\"latin1\")\n",
    "\n",
    "# Display first few rows\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "a1983fe1-bed0-4caa-946a-9dd6951695d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  label                                            message\n",
      "0   ham  Go until jurong point, crazy.. Available only ...\n",
      "1   ham                      Ok lar... Joking wif u oni...\n",
      "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
      "3   ham  U dun say so early hor... U c already then say...\n",
      "4   ham  Nah I don't think he goes to usf, he lives aro...\n"
     ]
    }
   ],
   "source": [
    "# Rename columns for clarity\n",
    "df = df.rename(columns={\"v1\": \"label\", \"v2\": \"message\"})\n",
    "\n",
    "# Drop unnecessary columns\n",
    "df = df[[\"label\", \"message\"]]\n",
    "\n",
    "# Display the cleaned dataset\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "2fae1917-1565-4b24-bae0-1683d9b5ce5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label      0\n",
      "message    0\n",
      "dtype: int64\n",
      "Duplicate messages: 403\n",
      "label\n",
      "ham     4825\n",
      "spam     747\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Check for missing values\n",
    "print(df.isnull().sum())\n",
    "\n",
    "# Check for duplicate messages\n",
    "print(\"Duplicate messages:\", df.duplicated().sum())\n",
    "\n",
    "# Check class distribution\n",
    "print(df['label'].value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "93b475ce-8dea-45bc-b72e-95fbe73c8b14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Remaining messages after duplicate removal: 5169\n"
     ]
    }
   ],
   "source": [
    "# Remove duplicate messages\n",
    "df = df.drop_duplicates(subset=\"message\")\n",
    "\n",
    "# Verify\n",
    "print(\"Remaining messages after duplicate removal:\", df.shape[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "52664c37-f9ba-410f-9108-9a84505c4272",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label      0\n",
      "message    0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(df.isnull().sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "bca470c3-0779-436f-b9b2-20437601e31e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   label                                            message\n",
      "0      0  Go until jurong point, crazy.. Available only ...\n",
      "1      0                      Ok lar... Joking wif u oni...\n",
      "2      1  Free entry in 2 a wkly comp to win FA Cup fina...\n",
      "3      0  U dun say so early hor... U c already then say...\n",
      "4      0  Nah I don't think he goes to usf, he lives aro...\n"
     ]
    }
   ],
   "source": [
    "df[\"label\"] = df[\"label\"].map({\"ham\": 0, \"spam\": 1})\n",
    "\n",
    "# Verify\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abe510a0-0ac5-454f-bc9b-9c7084592dc7",
   "metadata": {},
   "source": [
    "### text preprocessing - lowercase , removing special characters etc etc "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "6dbc4412-c51c-4816-92f2-42499db6be10",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\patil\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                             message  \\\n",
      "0  Go until jurong point, crazy.. Available only ...   \n",
      "1                      Ok lar... Joking wif u oni...   \n",
      "2  Free entry in 2 a wkly comp to win FA Cup fina...   \n",
      "3  U dun say so early hor... U c already then say...   \n",
      "4  Nah I don't think he goes to usf, he lives aro...   \n",
      "\n",
      "                                     cleaned_message  \n",
      "0  go jurong point crazi avail bugi n great world...  \n",
      "1                              ok lar joke wif u oni  \n",
      "2  free entri wkli comp win fa cup final tkt st m...  \n",
      "3                u dun say earli hor u c alreadi say  \n",
      "4               nah think goe usf live around though  \n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "# Download stopwords if not already downloaded\n",
    "nltk.download(\"stopwords\")\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "\n",
    "def preprocess_text(text):\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    # Remove special characters and numbers\n",
    "    text = re.sub(r\"[^a-zA-Z]\", \" \", text)\n",
    "    # Tokenize words\n",
    "    words = text.split()\n",
    "    # Remove stopwords and apply stemming\n",
    "    words = [stemmer.stem(word) for word in words if word not in stop_words]\n",
    "    # Join words back into a string\n",
    "    return \" \".join(words)\n",
    "\n",
    "# Apply preprocessing\n",
    "df[\"cleaned_message\"] = df[\"message\"].apply(preprocess_text)\n",
    "\n",
    "# Verify\n",
    "print(df[[\"message\", \"cleaned_message\"]].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "ad9a6562-c224-44d6-825c-7609fc9729c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TfidfVectorizer(max_features=5000)\n"
     ]
    }
   ],
   "source": [
    "print(vectorizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "dea6883c-970a-4a3d-bf06-804bda59a094",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X_new: (5169, 301)\n",
      "Shape of y_new: (5169,)\n"
     ]
    }
   ],
   "source": [
    "# Transform the messages using the already fitted vectorizer\n",
    "X_new = vectorizer.transform(df['message']).toarray()  \n",
    "\n",
    "# Store the labels (Spam = 1, Ham = 0)\n",
    "y_new = df['label'].values  \n",
    "\n",
    "# Print the shape to check dimensions\n",
    "print(\"Shape of X_new:\", X_new.shape)\n",
    "print(\"Shape of y_new:\", y_new.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "24b2bf59-28af-4f5f-8a28-9ea211ec222f",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'classifier' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[78], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Predict on new dataset\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m y_pred_new \u001b[38;5;241m=\u001b[39m classifier\u001b[38;5;241m.\u001b[39mpredict(X_new)\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Evaluate performance\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m classification_report\n",
      "\u001b[1;31mNameError\u001b[0m: name 'classifier' is not defined"
     ]
    }
   ],
   "source": [
    "# Predict on new dataset\n",
    "y_pred_new = classifier.predict(X_new)\n",
    "\n",
    "# Evaluate performance\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_new, y_pred_new))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "66969102-c12e-46a0-97a1-0e7767f36d32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.53      0.66      4516\n",
      "           1       0.14      0.55      0.23       653\n",
      "\n",
      "    accuracy                           0.53      5169\n",
      "   macro avg       0.52      0.54      0.45      5169\n",
      "weighted avg       0.80      0.53      0.61      5169\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import joblib\n",
    "\n",
    "# Load the saved classifier\n",
    "classifier = joblib.load(\"spam_classifier.pkl\")  # Update the filename if needed\n",
    "\n",
    "# Predict on new dataset\n",
    "y_pred_new = classifier.predict(X_new)\n",
    "\n",
    "# Evaluate performance\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_new, y_pred_new))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c539dd20-c8aa-4509-8ea0-441ced036a90",
   "metadata": {},
   "source": [
    "### Refit TF-IDF on the Larger Dataset - bcoz we had only 301 features in the smaller dataset so lets retrain tf-idf on larger dataset and then retrain on larger dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "97a5976d-f2fb-4569-9744-af56a4659853",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X_new: (5169, 5000)\n",
      "Shape of y_new: (5169,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Reinitialize and fit TF-IDF on the full dataset\n",
    "vectorizer = TfidfVectorizer(max_features=5000)  # Using 5000 features to generalize better\n",
    "X_new = vectorizer.fit_transform(df['message']).toarray()  # Transform all messages\n",
    "\n",
    "# Store updated labels\n",
    "y_new = df['label'].values\n",
    "\n",
    "# Check new shape\n",
    "print(\"Shape of X_new:\", X_new.shape)\n",
    "print(\"Shape of y_new:\", y_new.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "653650ed-59e4-4353-8b8a-2b240facb5b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy: 0.973639661426844\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "# Split the new dataset into training & testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_new, y_new, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize and train Naive Bayes classifier\n",
    "nb_classifier = MultinomialNB()\n",
    "nb_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate accuracy on training data\n",
    "train_accuracy = nb_classifier.score(X_train, y_train)\n",
    "print(\"Training Accuracy:\", train_accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "e396e1e5-b331-4da4-8c98-cb272774fe9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      1.00      0.98       889\n",
      "           1       1.00      0.77      0.87       145\n",
      "\n",
      "    accuracy                           0.97      1034\n",
      "   macro avg       0.98      0.89      0.93      1034\n",
      "weighted avg       0.97      0.97      0.97      1034\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Predict on test set\n",
    "y_pred_new = nb_classifier.predict(X_test)\n",
    "\n",
    "# Generate classification report\n",
    "print(classification_report(y_test, y_pred_new))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "23954dbe-f422-40f0-ac04-f950c2d3b821",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model and vectorizer saved successfully!\n"
     ]
    }
   ],
   "source": [
    "import joblib\n",
    "\n",
    "# Save the trained classifier\n",
    "joblib.dump(nb_classifier, \"spam_classifier.pkl\")\n",
    "\n",
    "# Save the TF-IDF vectorizer\n",
    "joblib.dump(vectorizer, \"tfidf_vectorizer.pkl\")\n",
    "\n",
    "print(\"Model and vectorizer saved successfully!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49669cbd-c38a-42dd-8d81-e79acd8633e3",
   "metadata": {},
   "source": [
    "### predicting with an example "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "358acc84-90af-4343-9f36-27b226fda2b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spam\n"
     ]
    }
   ],
   "source": [
    "# Load the saved classifier and vectorizer\n",
    "loaded_classifier = joblib.load(\"spam_classifier.pkl\")\n",
    "loaded_vectorizer = joblib.load(\"tfidf_vectorizer.pkl\")\n",
    "\n",
    "# Example new message\n",
    "new_message = [\"Congratulations! You won a free iPhone! Claim now.\"]\n",
    "\n",
    "# Transform using saved vectorizer\n",
    "new_message_transformed = loaded_vectorizer.transform(new_message).toarray()\n",
    "\n",
    "# Predict\n",
    "prediction = loaded_classifier.predict(new_message_transformed)\n",
    "\n",
    "# Output result\n",
    "print(\"Spam\" if prediction[0] == 1 else \"Ham\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "a9142efd-0b25-4a73-b85e-b9975b083f89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ham\n"
     ]
    }
   ],
   "source": [
    "# Example ham message\n",
    "ham_message = [\"Hey, what are you doing ?\"]\n",
    "\n",
    "# Transform using saved vectorizer\n",
    "ham_message_transformed = loaded_vectorizer.transform(ham_message).toarray()\n",
    "\n",
    "# Predict\n",
    "ham_prediction = loaded_classifier.predict(ham_message_transformed)\n",
    "\n",
    "# Output result\n",
    "print(\"Spam\" if ham_prediction[0] == 1 else \"Ham\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "516639be-fe70-4de4-8174-ba2d7f1b2f01",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
